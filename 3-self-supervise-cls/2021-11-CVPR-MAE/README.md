# MAE



> [论文地址](https://arxiv.org/abs/2111.06377)



## 1. 简介



### 1.1 简介

transformer 是纯基于注意力机制的编码器和解码器的架构

**论文成果简介**

MAE这篇文章就是BERT的CV版本



此文最大的贡献，可能是在NLP和CV两大领域之间架起了一座更简便的桥梁。

此前，大名鼎鼎的GPT和BERT已经将大型自然语言处理（NLP）模型的性能提升到了一个新的高度。

直观点讲，就是事先遮住一些文本片段，让AI模型通过自监督学习，通过海量语料库的预训练，逐步掌握上下文语境，把这些被遮住的片段，用尽可能合乎逻辑的方式填回去。

这和我们做「完形填空」的方式有些类似。经过海量数据的学习和训练，AI模型慢慢学会了自己生成自然文本。目前，随着GPT及其后续改进模型的不断进步，生成的自然文本几乎可以乱真。

现在，何恺明的这篇文章把NLP领域已被证明极其有效的方式，用在了计算机视觉（CV）领域，而且模型更简单。





### 1.2 





## 2. 网络架构



